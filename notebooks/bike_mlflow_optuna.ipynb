{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivo do Projeto\n",
    "\n",
    "O objetivo do projeto é demonstrar a aplicação das ferramentas de tracking do MLflow, otimização de hiperparametros com Optuna aplicado no modelo XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from eda_utils import plot_correlation_with_demand\n",
    "from eda_utils import plot_feature_importance\n",
    "from eda_utils import plot_residuals\n",
    "import xgboost as xgb\n",
    "import mlflow\n",
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from modeling_utils import champion_callback\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv('../data/raw/bike.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Exploratória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte a coluna 'dteday' para o tipo datetime\n",
    "data['dteday'] = pd.to_datetime(data['dteday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reajuste da demanda pela média de aluguel por dia.\n",
    "data.set_index('dteday')['cnt'].resample('D').mean().plot(title='Demanda de Bikes');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para verificar a correlação das variáveis independentes com a target\n",
    "correlation_plot = plot_correlation_with_demand(data, save_path=\"../report/figures/correlation_plot.png\")\n",
    "correlation_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Matriz de Correlação', fontsize=18)\n",
    "sns.heatmap(data.corr(), annot=True, fmt=\".1f\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão 80% treino e 20% para validação.\n",
    "train = data.loc[data['dteday']< '2012-08-10']\n",
    "valid = data.loc[data['dteday']>= '2012-08-10']\n",
    "\n",
    "# Porcentagem dos dados de treino\n",
    "round(train.shape[0] / data.shape[0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo as variáveis 'instant', 'dteday'\n",
    "train.drop(['instant', 'dteday'], axis=1, inplace=True)\n",
    "valid.drop(['instant', 'dteday'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva os datasets pré-processados\n",
    "train.to_csv('../data/processed/train.csv',index=False)\n",
    "valid.to_csv('../data/processed/valid.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando as variáveis preditoras da variável target\n",
    "train_x = train.drop(columns='cnt')\n",
    "train_y = train['cnt']\n",
    "valid_x = valid.drop(columns='cnt')\n",
    "valid_y = valid['cnt']\n",
    "\n",
    "# XGBoost Data Format\n",
    "dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "dvalid = xgb.DMatrix(valid_x, label=valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurando o MLflow Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o experimento no MLflow\n",
    "experiment_id = mlflow.create_experiment(\"Bike Demand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o experimento a ser usado\n",
    "mlflow.set_experiment(experiment_id=experiment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logging para gerenciar as saídas**\n",
    "\n",
    "A medida que realizamos o ajuste de hiperparâmetros com o Optuna, é essencial entender que o processo pode gerar uma infinidade de execuções. Na verdade, tantas que a saída padrão (stdout) do registrador padrão pode rapidamente ficar inundada, produzindo páginas e mais páginas de relatórios de registro.\n",
    "\n",
    "Embora a verbosidade da configuração de registro padrão seja inegavelmente valiosa durante a fase de desenvolvimento do código, iniciar um teste em grande escala pode resultar em uma quantidade avassaladora de informações. Considerando isso, registrar cada detalhe no stdout se torna menos prático, especialmente quando temos ferramentas dedicadas como o MLflow para rastrear meticulosamente nossos experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override Optuna's default logging to ERROR only\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Define hyperparameters\n",
    "        params = {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"eval_metric\": \"rmse\",\n",
    "            # defines booster, gblinear for linear functions.\n",
    "            \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "            # L2 regularization weight.\n",
    "            \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "            # L1 regularization weight.\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "            # sampling ratio for training data.\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "            # sampling according to each tree.\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "        }\n",
    "\n",
    "        if params[\"booster\"] == \"gbtree\" or params[\"booster\"] == \"dart\":\n",
    "            # maximum depth of the tree, signifies complexity of the tree.\n",
    "            params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n",
    "            # minimum child weight, larger the term more conservative the tree.\n",
    "            params[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "            # taxa de aprendizado\n",
    "            params[\"learning_rate\"] = trial.suggest_float(\"learning_rate\", 1e-8, 1.0, log=True)\n",
    "            # controla a poda\n",
    "            params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "\n",
    "        # Train XGBoost model\n",
    "        bst = xgb.train(params, dtrain)\n",
    "        preds = bst.predict(dvalid)\n",
    "        error = mean_squared_error(valid_y, preds)\n",
    "\n",
    "        # Log to MLflow\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"mse\", error)\n",
    "        mlflow.log_metric(\"rmse\", math.sqrt(error))\n",
    "\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrating Hyperparameter Tuning, Model Training, and Logging with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"first_attempt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the parent run and call the hyperparameter tuning child run logic\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name=run_name, nested=True):\n",
    "    # Initialize the Optuna study\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "    # Execute the hyperparameter optimization trials.\n",
    "    # Note the addition of the `champion_callback` inclusion to control our logging\n",
    "    study.optimize(objective, n_trials=500, callbacks=[champion_callback])\n",
    "\n",
    "    mlflow.log_params(study.best_params)\n",
    "    mlflow.log_metric(\"best_mse\", study.best_value)\n",
    "    mlflow.log_metric(\"best_rmse\", math.sqrt(study.best_value))\n",
    "\n",
    "    # Log tags\n",
    "    mlflow.set_tags(\n",
    "        tags={\n",
    "            \"project\": \"Bike Demand Project\",\n",
    "            \"optimizer_engine\": \"optuna\",\n",
    "            \"model_family\": \"xgboost\",\n",
    "            \"feature_set_version\": 1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Log a fit model instance\n",
    "    model = xgb.train(study.best_params, dtrain)\n",
    "\n",
    "    # Log the correlation plot\n",
    "    mlflow.log_figure(figure=correlation_plot, artifact_file=\"correlation_plot.png\")\n",
    "\n",
    "    # Log the feature importances plot\n",
    "    importances = plot_feature_importance(model, booster=study.best_params.get(\"booster\"))\n",
    "    mlflow.log_figure(figure=importances, artifact_file=\"feature_importances.png\")\n",
    "\n",
    "    # Log the residuals plot\n",
    "    residuals = plot_residuals(model, dvalid, valid_y)\n",
    "    mlflow.log_figure(figure=residuals, artifact_file=\"residuals.png\")\n",
    "\n",
    "    artifact_path = \"model\"\n",
    "\n",
    "    mlflow.xgboost.log_model(\n",
    "        xgb_model=model,\n",
    "        artifact_path=artifact_path,\n",
    "        input_example=train_x.iloc[[0]],\n",
    "        model_format=\"ubj\",\n",
    "        metadata={\"model_data_version\": 1},\n",
    "    )\n",
    "\n",
    "    # Get the logged model uri so that we can load it from the artifact store\n",
    "    model_uri = mlflow.get_artifact_uri(artifact_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.trials_dataframe().sort_values(by='value', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Trained Model with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = mlflow.xgboost.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dmatrix = xgb.DMatrix(valid_x)\n",
    "\n",
    "inference = loaded.predict(batch_dmatrix)\n",
    "\n",
    "infer_data = valid.copy()\n",
    "\n",
    "infer_data[\"predicted_demand\"] = inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_data[\"predicted_demand\"] = infer_data[\"predicted_demand\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_data[['cnt','predicted_demand']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_data[infer_data['predicted_demand'] <= 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
